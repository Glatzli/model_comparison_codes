<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/plot_timeseries.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/plot_timeseries.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;This script is used to plot the time series of the vertical distribution of potential temperature for all models.&#10;problem: vertical coordinate is not the same for all models =&gt; use pressure?&#10;&quot;&quot;&quot;&#10;&#10;# Fix for OpenMP duplicate library error on Windows&#10;import os&#10;os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'&#10;&#10;import sys&#10;sys.path.append(&quot;D:/MSc_Arbeit/model_comparison_codes&quot;)&#10;import importlib&#10;import read_in_arome&#10;import read_icon_model_3D&#10;import read_ukmo&#10;# importlib.reload(read_icon_model_3D)&#10;import read_wrf_helen&#10;importlib.reload(read_in_arome)&#10;import confg&#10;import xarray as xr&#10;import numpy as np&#10;import matplotlib&#10;import matplotlib.pyplot as plt&#10;import pandas as pd&#10;from colorspace import diverging_hcl&#10;&#10;&#10;&#10;def plot_pot_temp_time_contours(pot_temp, model=&quot;AROME&quot;):&#10;    &quot;&quot;&quot;&#10;    plot pot temp time &amp; height series for all models. HATPRO was interpolated to AROME levels &amp; it's pressure is used&#10;    to compute pot temp.&#10;    thin 1 K pot temp contour lines, thick 5 K pot temp contour lines and red/blue shading for the 1/2 hrly&#10;    warming/cooling in pot temp is plotted&#10;&#10;    :param pot_temp:&#10;    :param model:&#10;    :return:&#10;    &quot;&quot;&quot;&#10;    fig, ax = plt.subplots(figsize=(12, 6))&#10;&#10;    vmin, vmax = -2, 2  # uniform colorbar&#10;    levels = np.arange(vmin, vmax + 0.5, 0.5)&#10;    # limit the time range for the plot&#10;    start_time = pd.to_datetime('2017-10-15 13:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    end_time = pd.to_datetime('2017-10-16 12:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    # pot_temp = pot_temp.sel(time=slice(start_time, end_time))&#10;&#10;    # Plot the filled contours&#10;    contourf = (pot_temp.diff(&quot;time&quot;, n=1) * 2).plot.contourf(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;, levels=levels, cmap=pal1.cmap(),&#10;                                                   add_colorbar=False, vmin=vmin, vmax=vmax)&#10;&#10;    # Plot the contour lines&#10;    contour1 = pot_temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;,&#10;                                     levels=np.arange(np.round(pot_temp.min()), np.round(pot_temp.max()), 1),&#10;                                     colors='black', linewidths=0.5)   #.isel(time=slice(1, 100))&#10;    contour5 = pot_temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;, levels=np.arange(290, np.round(pot_temp.max()), 5),&#10;                                     colors='black', linewidths=1.5)  #.isel(time=slice(1, 100))&#10;    ax.clabel(contour5)&#10;&#10;    ax.set_xlim(start_time, end_time)&#10;    # Add a colorbar&#10;    cbar = plt.colorbar(contourf, ax=ax)&#10;    cbar.set_label('K hr$^{-1}$')&#10;&#10;    ax.set_title(model + &quot; potential temp time series&quot;)&#10;    if model ==  &quot;HATPRO&quot;:&#10;        ax.set_ylabel(f&quot;height [m]&quot;)&#10;    elif model == &quot;ICON&quot; or model == &quot;ICON2TE&quot;:&#10;        ax.set_ylabel(f&quot;geometric height [m]&quot;)&#10;    else:&#10;        ax.set_ylabel(f&quot;geopotential height [m]&quot;)&#10;    ax.set_xlabel(&quot;&quot;)&#10;&#10;    plt.savefig(confg.dir_PLOTS + model + f&quot;_pot_temp_timeseries_{interface_height}_ibk.png&quot;, dpi=500)&#10;&#10;&#10;def plot_temp_time_contours(temp, model=&quot;AROME&quot;):&#10;    &quot;&quot;&quot;&#10;    plot temp over time &amp; height for all models incl HATPRO.&#10;    :param temp:&#10;    :param model:&#10;    :return:&#10;    &quot;&quot;&quot;&#10;&#10;    fig, ax = plt.subplots(figsize=(12, 6))&#10;&#10;    vmin, vmax = -1.5, 1.5  # uniform colorbar&#10;    levels = np.arange(vmin, vmax + 0.3, 0.3)&#10;&#10;    start_time = pd.to_datetime('2017-10-15 14:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    end_time = pd.to_datetime('2017-10-16 12:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    # temp = temp.sel(time=slice(start_time, end_time))&#10;&#10;    # Plot the filled contours&#10;    contourf = temp.diff(&quot;time&quot;).plot.contourf(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;, levels=levels, cmap=pal1.cmap(),&#10;                                                   add_colorbar=False, vmin=-2, vmax=2)&#10;&#10;    # Plot the contour lines&#10;    contour1 = temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;,&#10;                                                                    levels=np.arange(np.round(temp.min()),&#10;                                                                                     np.round(temp.max()), 1),&#10;                                                                    colors='black', linewidths=0.5)   #.isel(time=slice(1, 100))&#10;    contour5 = temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;,&#10;                                                                    levels=np.arange(-50, np.round(temp.max()), 5),&#10;                                                                    colors='black', linewidths=1.5)  #.isel(time=slice(1, 100))&#10;    ax.clabel(contour5)&#10;&#10;    ax.set_xlim(start_time, end_time)&#10;    # Add a colorbar&#10;    cbar = plt.colorbar(contourf, ax=ax)&#10;    cbar.set_label('K hr$^{-1}$')&#10;    ax.set_title(model + &quot; temp time series&quot;)&#10;    if model ==  &quot;HATPRO&quot;:&#10;        ax.set_ylabel(f&quot;height [m]&quot;)&#10;    else:&#10;        ax.set_ylabel(f&quot;geopotential height [m]&quot;)&#10;    ax.set_xlabel(&quot;&quot;)&#10;    plt.savefig(confg.dir_PLOTS + model + &quot;_temp_timeseries_ibk.png&quot;, dpi=300)&#10;    plt.show()&#10;&#10;&#10;def plot_arome():&#10;    # arome = read_in_arome.read_in_arome_fixed_point(lat=lat_ibk, lon=lon_ibk, )&#10;    # arome = read_in_arome.read_3D_variables_AROME(variables=[&quot;p&quot;, &quot;th&quot;, &quot;z&quot;], method=&quot;sel&quot;, lat=lat_ibk, lon=lon_ibk)&#10;    # pot_temp = arome.th.isel(nz=np.arange(40, 90))&#10;&#10;    arome = xr.open_dataset(confg.model_folder + &quot;/AROME/&quot; + &quot;AROME_temp_timeseries_ibk.nc&quot;)&#10;    pot_temp = arome.th.where(arome[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;    plot_pot_temp_time_contours(pot_temp, model=&quot;AROME&quot;)&#10;&#10;    # temp = arome.temperature.where(arome.height &lt;= 4000, drop=True)  # tried with normal temp, but you don't see much...&#10;    # plot_temp_time_contours(temp, model=&quot;AROME&quot;)&#10;&#10;def plot_icon():&#10;    &quot;&quot;&quot;icon15 = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=15, hours=np.arange(14, 23), lon=lon_ibk, lat=lat_ibk, variant=&quot;ICON&quot;)&#10;    icon16 = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=16, hours=np.arange(0, 9), lon=lon_ibk, lat=lat_ibk, variant=&quot;ICON&quot;)&#10;    variables = [&quot;th&quot;, &quot;temp&quot;, &quot;z_ifc&quot;]  # &quot;temp&quot;, &quot;pres&quot;, &quot;u&quot;, &quot;v&quot;, &quot;w&quot;,&#10;    icon = xr.concat([icon15[variables], icon16[variables]], dim=&quot;time&quot;)&quot;&quot;&quot;&#10;    icon = xr.open_dataset(confg.icon_folder_3D + &quot;/ICON_temp_timeseries_ibk.nc&quot;)&#10;    pot_temp = icon.th.where(icon[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;    plot_pot_temp_time_contours(pot_temp, model=&quot;ICON&quot;)&#10;&#10;def plot_icon2te():&#10;    &quot;&quot;&quot;icon15_2te = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=15, hours=range(12, 24), lon=lon_ibk,&#10;                                                                         lat=lat_ibk, variant=&quot;ICON2TE&quot;)&#10;    icon16_2te = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=16, hours=range(00, 13), lon=lon_ibk,&#10;                                                                         lat=lat_ibk, variant=&quot;ICON2TE&quot;)&#10;    variables = [&quot;th&quot;, &quot;temp&quot;]  # [&quot;temp&quot;, &quot;pressure&quot;, &quot;pres&quot;, &quot;u&quot;, &quot;v&quot;, &quot;w&quot;]&#10;    icon2te = xr.concat([icon15_2te[variables], icon16_2te[variables]], dim=&quot;time&quot;)&#10;    icon2te_pot_temp = icon2te.th.isel(height=np.arange(40, 90))&quot;&quot;&quot;&#10;&#10;    icon_2te = xr.open_dataset(confg.icon2TE_folder_3D + &quot;/ICON_2TE_latlon_temp_timeseries_ibk.nc&quot;)&#10;    icon_2te_pot_temp = icon_2te.th.where(icon_2te[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;    plot_pot_temp_time_contours(pot_temp=icon_2te_pot_temp, model=&quot;ICON2TE_latlon&quot;)&#10;&#10;def plot_ukmo():&#10;    um = xr.open_dataset(confg.ukmo_folder + &quot;/UKMO_temp_timeseries_ibk.nc&quot;)&#10;    um_pot_temp = um.th.where(um[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;&#10;    plot_pot_temp_time_contours(pot_temp=um_pot_temp, model=&quot;UKMO&quot;)&#10;&#10;def plot_wrf():&#10;    # wrf = read_wrf_helen.read_wrf_fixed_point(lat=lat_ibk, lon=lon_ibk)&#10;    # wrf_pot_temp = wrf.th.isel(height=slice(0, 50))&#10;&#10;    wrf = xr.open_dataset(confg.wrf_folder + &quot;/WRF_temp_timeseries_ibk.nc&quot;)&#10;    wrf_pot_temp = wrf.th.where(wrf[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;&#10;    plot_pot_temp_time_contours(pot_temp=wrf_pot_temp, model=&quot;WRF&quot;)&#10;&#10;&#10;def plot_hatpro():&#10;    # hatpro = xr.open_dataset(f&quot;{confg.hatpro_folder}/hatpro_merged.nc&quot;)&#10;    # hatpro_temp = hatpro[&quot;temperature&quot;].sel(height=slice(0, 4400))  # select up to 4.400 m&#10;    # plot_temp_time_contours(temp=hatpro_temp, model=&quot;HATPRO&quot;)&#10;    # hatpro&#10;&#10;    # try with hatpro interpolated data&#10;    hatpro = xr.open_dataset(f&quot;{confg.hatpro_folder}/hatpro_interpolated_arome.nc&quot;)&#10;    hatpro_pot_temp = hatpro[&quot;th&quot;].where(hatpro[&quot;height&quot;] &lt;= interface_height, drop=True)  # hatpro[&quot;th&quot;].sel(height=slice(0, 4400))  # select up to 4.400 m&#10;    plot_pot_temp_time_contours(pot_temp=hatpro_pot_temp, model=&quot;HATPRO&quot;)&#10;&#10;if __name__ == '__main__':&#10;    lat_ibk = 47.259998&#10;    lon_ibk = 11.384167&#10;    interface_height = 2500  # what is max height that should be plotted?&#10;    pal1 = diverging_hcl(palette=&quot;Blue-Red 2&quot;)&#10;&#10;    matplotlib.use('Qt5Agg')  # Use the Qt5Agg backend for interactive plotting&#10;&#10;    #plot_arome()&#10;&#10;    plot_icon()&#10;    plot_icon2te()&#10;&#10;    #plot_ukmo()&#10;    #plot_wrf()&#10;    #plot_hatpro()&#10;    plt.show()&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;This script is used to plot the time series of the vertical distribution of potential temperature for all models.&#10;problem: vertical coordinate is not the same for all models =&gt; use pressure?&#10;&quot;&quot;&quot;&#10;&#10;# Fix for OpenMP duplicate library error on Windows&#10;import os&#10;os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'&#10;&#10;import sys&#10;sys.path.append(&quot;D:/MSc_Arbeit/model_comparison_codes&quot;)&#10;import importlib&#10;import read_in_arome&#10;import read_icon_model_3D&#10;import read_ukmo&#10;# importlib.reload(read_icon_model_3D)&#10;import read_wrf_helen&#10;importlib.reload(read_in_arome)&#10;import confg&#10;import xarray as xr&#10;import numpy as np&#10;import matplotlib&#10;import matplotlib.pyplot as plt&#10;import pandas as pd&#10;from colorspace import diverging_hcl&#10;&#10;&#10;&#10;def plot_pot_temp_time_contours(pot_temp, model=&quot;AROME&quot;):&#10;    &quot;&quot;&quot;&#10;    plot pot temp time &amp; height series for all models. HATPRO was interpolated to AROME levels &amp; it's pressure is used&#10;    to compute pot temp.&#10;    thin 1 K pot temp contour lines, thick 5 K pot temp contour lines and red/blue shading for the 1/2 hrly&#10;    warming/cooling in pot temp is plotted&#10;&#10;    :param pot_temp:&#10;    :param model:&#10;    :return:&#10;    &quot;&quot;&quot;&#10;    fig, ax = plt.subplots(figsize=(12, 6))&#10;&#10;    vmin, vmax = -2, 2  # uniform colorbar&#10;    levels = np.arange(vmin, vmax + 0.5, 0.5)&#10;    # limit the time range for the plot&#10;    start_time = pd.to_datetime('2017-10-15 13:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    end_time = pd.to_datetime('2017-10-16 12:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    # pot_temp = pot_temp.sel(time=slice(start_time, end_time))&#10;&#10;    # Plot the filled contours&#10;    contourf = (pot_temp.diff(&quot;time&quot;, n=1) * 2).plot.contourf(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;, levels=levels, cmap=pal1.cmap(),&#10;                                                   add_colorbar=False, vmin=vmin, vmax=vmax)&#10;&#10;    # Plot the contour lines&#10;    contour1 = pot_temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;,&#10;                                     levels=np.arange(np.round(pot_temp.min()), np.round(pot_temp.max()), 1),&#10;                                     colors='black', linewidths=0.5)   #.isel(time=slice(1, 100))&#10;    contour5 = pot_temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;, levels=np.arange(290, np.round(pot_temp.max()), 5),&#10;                                     colors='black', linewidths=1.5)  #.isel(time=slice(1, 100))&#10;    ax.clabel(contour5)&#10;&#10;    ax.set_xlim(start_time, end_time)&#10;    # Add a colorbar&#10;    cbar = plt.colorbar(contourf, ax=ax)&#10;    cbar.set_label('K hr$^{-1}$')&#10;&#10;    ax.set_title(model + &quot; potential temp time series&quot;)&#10;    if model ==  &quot;HATPRO&quot;:&#10;        ax.set_ylabel(f&quot;height [m]&quot;)&#10;    elif model == &quot;ICON&quot; or model == &quot;ICON2TE&quot;:&#10;        ax.set_ylabel(f&quot;geometric height [m]&quot;)&#10;    else:&#10;        ax.set_ylabel(f&quot;geopotential height [m]&quot;)&#10;    ax.set_xlabel(&quot;&quot;)&#10;&#10;    plt.savefig(confg.dir_PLOTS + model + f&quot;_pot_temp_timeseries_{interface_height}_ibk.png&quot;, dpi=500)&#10;&#10;&#10;def plot_temp_time_contours(temp, model=&quot;AROME&quot;):&#10;    &quot;&quot;&quot;&#10;    plot temp over time &amp; height for all models incl HATPRO.&#10;    :param temp:&#10;    :param model:&#10;    :return:&#10;    &quot;&quot;&quot;&#10;&#10;    fig, ax = plt.subplots(figsize=(12, 6))&#10;&#10;    vmin, vmax = -1.5, 1.5  # uniform colorbar&#10;    levels = np.arange(vmin, vmax + 0.3, 0.3)&#10;&#10;    start_time = pd.to_datetime('2017-10-15 14:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    end_time = pd.to_datetime('2017-10-16 12:00:00', format='%Y-%m-%d %H:%M:%S')&#10;    # temp = temp.sel(time=slice(start_time, end_time))&#10;&#10;    # Plot the filled contours&#10;    contourf = temp.diff(&quot;time&quot;).plot.contourf(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;, levels=levels, cmap=pal1.cmap(),&#10;                                                   add_colorbar=False, vmin=-2, vmax=2)&#10;&#10;    # Plot the contour lines&#10;    contour1 = temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;,&#10;                                                                    levels=np.arange(np.round(temp.min()),&#10;                                                                                     np.round(temp.max()), 1),&#10;                                                                    colors='black', linewidths=0.5)   #.isel(time=slice(1, 100))&#10;    contour5 = temp.plot.contour(ax=ax, x=&quot;time&quot;, y=&quot;height&quot;,&#10;                                                                    levels=np.arange(-50, np.round(temp.max()), 5),&#10;                                                                    colors='black', linewidths=1.5)  #.isel(time=slice(1, 100))&#10;    ax.clabel(contour5)&#10;&#10;    ax.set_xlim(start_time, end_time)&#10;    # Add a colorbar&#10;    cbar = plt.colorbar(contourf, ax=ax)&#10;    cbar.set_label('K hr$^{-1}$')&#10;    ax.set_title(model + &quot; temp time series&quot;)&#10;    if model ==  &quot;HATPRO&quot;:&#10;        ax.set_ylabel(f&quot;height [m]&quot;)&#10;    else:&#10;        ax.set_ylabel(f&quot;geopotential height [m]&quot;)&#10;    ax.set_xlabel(&quot;&quot;)&#10;    plt.savefig(confg.dir_PLOTS + model + &quot;_temp_timeseries_ibk.png&quot;, dpi=300)&#10;    plt.show()&#10;&#10;&#10;def plot_arome():&#10;    # arome = read_in_arome.read_in_arome_fixed_point(lat=lat_ibk, lon=lon_ibk, )&#10;    # arome = read_in_arome.read_3D_variables_AROME(variables=[&quot;p&quot;, &quot;th&quot;, &quot;z&quot;], method=&quot;sel&quot;, lat=lat_ibk, lon=lon_ibk)&#10;    # pot_temp = arome.th.isel(nz=np.arange(40, 90))&#10;&#10;    arome = xr.open_dataset(confg.model_folder + &quot;/AROME/&quot; + &quot;AROME_temp_timeseries_ibk.nc&quot;)&#10;    pot_temp = arome.th.where(arome[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;    plot_pot_temp_time_contours(pot_temp, model=&quot;AROME&quot;)&#10;&#10;    # temp = arome.temperature.where(arome.height &lt;= 4000, drop=True)  # tried with normal temp, but you don't see much...&#10;    # plot_temp_time_contours(temp, model=&quot;AROME&quot;)&#10;&#10;def plot_icon():&#10;    &quot;&quot;&quot;icon15 = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=15, hours=np.arange(14, 23), lon=lon_ibk, lat=lat_ibk, variant=&quot;ICON&quot;)&#10;    icon16 = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=16, hours=np.arange(0, 9), lon=lon_ibk, lat=lat_ibk, variant=&quot;ICON&quot;)&#10;    variables = [&quot;th&quot;, &quot;temp&quot;, &quot;z_ifc&quot;]  # &quot;temp&quot;, &quot;pres&quot;, &quot;u&quot;, &quot;v&quot;, &quot;w&quot;,&#10;    icon = xr.concat([icon15[variables], icon16[variables]], dim=&quot;time&quot;)&quot;&quot;&quot;&#10;    icon = xr.open_dataset(confg.icon_folder_3D + &quot;/ICON_temp_timeseries_ibk.nc&quot;)&#10;    pot_temp = icon.th.where(icon[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;    plot_pot_temp_time_contours(pot_temp, model=&quot;ICON&quot;)&#10;&#10;def plot_icon2te():&#10;    &quot;&quot;&quot;icon15_2te = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=15, hours=range(12, 24), lon=lon_ibk,&#10;                                                                         lat=lat_ibk, variant=&quot;ICON2TE&quot;)&#10;    icon16_2te = read_icon_model_3D.read_icon_fixed_point_multiple_hours(day=16, hours=range(00, 13), lon=lon_ibk,&#10;                                                                         lat=lat_ibk, variant=&quot;ICON2TE&quot;)&#10;    variables = [&quot;th&quot;, &quot;temp&quot;]  # [&quot;temp&quot;, &quot;pressure&quot;, &quot;pres&quot;, &quot;u&quot;, &quot;v&quot;, &quot;w&quot;]&#10;    icon2te = xr.concat([icon15_2te[variables], icon16_2te[variables]], dim=&quot;time&quot;)&#10;    icon2te_pot_temp = icon2te.th.isel(height=np.arange(40, 90))&quot;&quot;&quot;&#10;&#10;    icon_2te = xr.open_dataset(confg.icon2TE_folder_3D + &quot;/ICON_2TE_latlon_temp_timeseries_ibk.nc&quot;)&#10;    icon_2te_pot_temp = icon_2te.th.where(icon_2te[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;    plot_pot_temp_time_contours(pot_temp=icon_2te_pot_temp, model=&quot;ICON2TE_latlon&quot;)&#10;&#10;def plot_ukmo():&#10;    um = xr.open_dataset(confg.ukmo_folder + &quot;/UKMO_temp_timeseries_ibk.nc&quot;)&#10;    um_pot_temp = um.th.where(um[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;&#10;    plot_pot_temp_time_contours(pot_temp=um_pot_temp, model=&quot;UKMO&quot;)&#10;&#10;def plot_wrf():&#10;    # wrf = read_wrf_helen.read_wrf_fixed_point(lat=lat_ibk, lon=lon_ibk)&#10;    # wrf_pot_temp = wrf.th.isel(height=slice(0, 50))&#10;&#10;    wrf = xr.open_dataset(confg.wrf_folder + &quot;/WRF_temp_timeseries_ibk.nc&quot;)&#10;    wrf_pot_temp = wrf.th.where(wrf[&quot;height&quot;] &lt;= interface_height, drop=True)&#10;&#10;    plot_pot_temp_time_contours(pot_temp=wrf_pot_temp, model=&quot;WRF&quot;)&#10;&#10;&#10;def plot_hatpro():&#10;    # hatpro = xr.open_dataset(f&quot;{confg.hatpro_folder}/hatpro_merged.nc&quot;)&#10;    # hatpro_temp = hatpro[&quot;temperature&quot;].sel(height=slice(0, 4400))  # select up to 4.400 m&#10;    # plot_temp_time_contours(temp=hatpro_temp, model=&quot;HATPRO&quot;)&#10;    # hatpro&#10;&#10;    # try with hatpro interpolated data&#10;    hatpro = xr.open_dataset(f&quot;{confg.hatpro_folder}/hatpro_interpolated_arome.nc&quot;)&#10;    hatpro_pot_temp = hatpro[&quot;th&quot;].where(hatpro[&quot;height&quot;] &lt;= interface_height, drop=True)  # hatpro[&quot;th&quot;].sel(height=slice(0, 4400))  # select up to 4.400 m&#10;    plot_pot_temp_time_contours(pot_temp=hatpro_pot_temp, model=&quot;HATPRO&quot;)&#10;&#10;if __name__ == '__main__':&#10;    lat_ibk = 47.259998&#10;    lon_ibk = 11.384167&#10;    interface_height = 2500  # what is max height that should be plotted?&#10;    pal1 = diverging_hcl(palette=&quot;Blue-Red 2&quot;)&#10;&#10;    matplotlib.use('Qt5Agg')  # Use the Qt5Agg backend for interactive plotting&#10;&#10;    #plot_arome()&#10;&#10;    plot_icon()&#10;    plot_icon2te()&#10;&#10;    #plot_ukmo()&#10;    #plot_wrf()&#10;    #plot_hatpro()&#10;    plt.show()" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/read_lidar.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/read_lidar.py" />
              <option name="originalContent" value="import glob&#10;import os&#10;&#10;import numpy as np&#10;import pandas as pd&#10;import plotly.graph_objects as go&#10;import xarray as xr&#10;from plotly.subplots import make_subplots&#10;&#10;import confg&#10;&#10;&#10;def read_edit_original_lidar_data(data_path, instrument_name):&#10;    &quot;&quot;&quot;&#10;    Reads LIDAR data from specified path, merges both files, converts time to datetime coordinates,&#10;    filters to desired time window and 30-min intervals, then saves as merged file&#10;&#10;    Parameters:&#10;    -----------&#10;    data_path : str&#10;        Path to LIDAR data folder&#10;    instrument_name : str&#10;        Instrument name (e.g. 'SL88' or 'SLXR142')&#10;&#10;    Returns:&#10;    --------&#10;    xarray.Dataset&#10;        Processed and saved dataset with datetime coordinates, filtered to time window and 30-min intervals&#10;    &quot;&quot;&quot;&#10;&#10;    # Find all NetCDF files in folder (exclude merged files)&#10;    nc_files = [f for f in glob.glob(os.path.join(data_path, &quot;*.nc&quot;))&#10;                if 'merged' not in os.path.basename(f)]&#10;&#10;    if not nc_files:&#10;        print(f&quot;No NetCDF files found in {data_path}&quot;)&#10;        return None&#10;&#10;    print(f&quot;Found {len(nc_files)} files for {instrument_name}&quot;)&#10;&#10;    try:&#10;        # Load all files and merge them&#10;        # Use decode_times=False to avoid cftime issues&#10;        datasets = []&#10;        height_array = None  # Store height array separately&#10;&#10;        for file in sorted(nc_files):&#10;            ds = xr.open_dataset(file, decode_times=False)&#10;&#10;            # Store height from first file&#10;            if height_array is None and 'height' in ds.variables:&#10;                height_array = ds['height'].values&#10;&#10;            # Add instrument info&#10;            ds.attrs['instrument'] = instrument_name&#10;            datasets.append(ds)&#10;&#10;        # Combine along time index dimension&#10;        combined_ds = xr.concat(datasets, dim='NUMBER_OF_SCANS')&#10;&#10;        if 'time' in combined_ds.variables:&#10;            # Convert time values to datetime&#10;            datetime_values = pd.to_datetime(combined_ds.time, unit='s')&#10;&#10;            # Remove old time variable&#10;            combined_ds = combined_ds.drop_vars(&quot;time&quot;)&#10;&#10;            # Rename dimension and set datetime as coordinate&#10;            combined_ds = combined_ds.rename({'NUMBER_OF_SCANS': 'time'})&#10;            combined_ds = combined_ds.assign_coords({'time': datetime_values})&#10;&#10;        # Add height as 1D coordinate (not as concatenated variable)&#10;        if height_array is not None:&#10;            combined_ds = combined_ds.assign_coords({'height_m': ('NUMBER_OF_GATES', height_array)})&#10;        &#10;        # Fix height coordinate: use first timestamp's height values as constant coordinate&#10;        if 'height' in combined_ds.variables and len(combined_ds['height'].dims) == 2:&#10;            print(f&quot;  Converting 2D height coordinate to 1D using first timestamp...&quot;)&#10;            # Get height values from first timestamp&#10;            first_timestamp_heights = combined_ds['height'].isel(time=0).values&#10;            &#10;            # Remove the old 2D height variable&#10;            combined_ds = combined_ds.drop_vars('height')&#10;            &#10;            # Add as 1D coordinate&#10;            combined_ds = combined_ds.assign_coords({'height': ('NUMBER_OF_GATES', first_timestamp_heights)})&#10;            print(f&quot;  Height coordinate converted from 2D to 1D with {len(first_timestamp_heights)} levels&quot;)&#10;&#10;        print(f&quot;  Full time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}&quot;)&#10;&#10;        # Define time window: 2017-10-15 12:00 to 2017-10-16 12:00&#10;        start_time = '2017-10-15 12:00:00'&#10;        end_time = '2017-10-16 12:00:00'&#10;&#10;        # Time window filtering first&#10;        filtered_ds = combined_ds.sel(time=slice(start_time, end_time))&#10;&#10;        # Create 30-min interval timestamps (aligned to :00 and :30)&#10;        target_times = pd.date_range(start=start_time, end=end_time, freq='30min')&#10;&#10;        print(f&quot;  Selecting nearest timesteps to 30-min intervals...&quot;)&#10;&#10;        # Select nearest timesteps to target times&#10;        try:&#10;            subset_ds = filtered_ds.sel(time=target_times, method='nearest', tolerance='10min')&#10;        except ValueError:&#10;            print()&#10;&#10;        print(f&quot;  After filtering: {len(subset_ds.time)} timesteps (30-min intervals)&quot;)&#10;        print(f&quot;  Final time range: {subset_ds.time.values[0]} to {subset_ds.time.values[-1]}&quot;)&#10;&#10;        # Rename NUMBER_OF_GATES dimension to height for clarity&#10;        if 'NUMBER_OF_GATES' in subset_ds.dims:&#10;            subset_ds = subset_ds.rename({'NUMBER_OF_GATES': 'height'})&#10;            print(f&quot;  Renamed dimension NUMBER_OF_GATES to height&quot;)&#10;        &#10;        # Ensure height coordinate is properly set as 1D after filtering&#10;        if 'height' in subset_ds.coords and len(subset_ds.coords['height'].dims) == 1:&#10;            print(f&quot;  Height coordinate is now 1D with {len(subset_ds.coords['height'])} levels&quot;)&#10;            print(f&quot;  Height range: {subset_ds.coords['height'].min().values:.2f} - {subset_ds.coords['height'].max().values:.2f} m&quot;)&#10;&#10;        # Save path - use appropriate filename based on instrument&#10;        if instrument_name == 'SL88':&#10;            output_path = os.path.join(data_path, 'sl88_merged.nc')&#10;        elif instrument_name == 'SLXR142':&#10;            output_path = os.path.join(data_path, 'slxr142_merged.nc')&#10;        else:&#10;            output_path = os.path.join(data_path, f'{instrument_name.lower()}_merged.nc')&#10;&#10;        # Save as NetCDF&#10;        subset_ds.to_netcdf(output_path)&#10;        print(f&quot;  Saved to: {output_path}&quot;)&#10;&#10;        return subset_ds&#10;&#10;    except Exception as e:&#10;        print(f&quot;Error loading {instrument_name} data: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return None&#10;&#10;&#10;def read_merged_lidar_data():&#10;    &quot;&quot;&quot;&#10;    Reads both pre-processed merged LIDAR files if they exist&#10;&#10;    Returns:&#10;    --------&#10;    tuple&#10;        (sl88_data, slxr142_data) - Two xarray Datasets or None if not available&#10;    &quot;&quot;&quot;&#10;    sl88_data, slxr142_data = None, None&#10;&#10;    # Check and load SL88 merged file&#10;    if os.path.exists(confg.lidar_sl88_merged_path):&#10;        print(f&quot;Loading existing SL88 merged file: {confg.lidar_sl88_merged_path}&quot;)&#10;        try:&#10;            sl88_data = xr.open_dataset(confg.lidar_sl88_merged_path)&#10;            print(&#10;                f&quot;  SL88: {len(sl88_data.time)} timesteps from {sl88_data.time.values[0]} to {sl88_data.time.values[-1]}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;Error loading SL88 merged file: {e}&quot;)&#10;            sl88_data = None&#10;    else:&#10;        print(f&quot;SL88 merged file not found: {confg.lidar_sl88_merged_path}&quot;)&#10;&#10;    # Check and load SLXR142 merged file&#10;    if os.path.exists(confg.lidar_slxr142_merged_path):&#10;        print(f&quot;Loading existing SLXR142 merged file: {confg.lidar_slxr142_merged_path}&quot;)&#10;        try:&#10;            slxr142_data = xr.open_dataset(confg.lidar_slxr142_merged_path)&#10;            print(&#10;                f&quot;  SLXR142: {len(slxr142_data.time)} timesteps from {slxr142_data.time.values[0]} to {slxr142_data.time.values[-1]}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;Error loading SLXR142 merged file: {e}&quot;)&#10;            slxr142_data = None&#10;    else:&#10;        print(f&quot;SLXR142 merged file not found: {confg.lidar_slxr142_merged_path}&quot;)&#10;&#10;    return sl88_data, slxr142_data&#10;&#10;&#10;def plot_lidar_comparison(sl88_data, slxr142_data, save_plot=True):&#10;    &quot;&quot;&quot;&#10;    Creates interactive Plotly comparison plots with time slider for wind speed and direction of both LIDAR systems&#10;    Data is expected to be already filtered to desired time range and 30-min intervals&#10;&#10;    Parameters:&#10;    -----------&#10;    sl88_data : xarray.Dataset&#10;        SL88 LIDAR data (with datetime coordinates, already filtered)&#10;    slxr142_data : xarray.Dataset&#10;        SLXR142 LIDAR data (with datetime coordinates, already filtered)&#10;    save_plot : bool&#10;        Whether to save the plot&#10;    &quot;&quot;&quot;&#10;&#10;    if sl88_data is None or slxr142_data is None:&#10;        print(&quot;One or both datasets are empty!&quot;)&#10;        return&#10;&#10;    # Colors for both systems&#10;    colors = {'SL88': '#1f77b4', 'SLXR142': '#ff7f0e'}&#10;    # extract heights&#10;    heights_sl88 = sl88_data['height_m'].values&#10;    heights_slxr142 = slxr142_data['height_m'].values&#10;&#10;    print(f&quot;SL88: {len(sl88_data.time)} timesteps from {sl88_data.time.values[0]} to {sl88_data.time.values[-1]}&quot;)&#10;    print(f&quot;SLXR142: {len(slxr142_data.time)} timesteps from {slxr142_data.time.values[0]} to {slxr142_data.time.values[-1]}&quot;)&#10;&#10;    # Use full time series for slider&#10;    max_frames = max(len(sl88_data.time), len(slxr142_data.time))&#10;&#10;    print(f&quot;Creating interactive plot with {max_frames} timesteps...&quot;)&#10;&#10;    # Create subplots: [Wind Speed | Wind Direction]&#10;    fig = make_subplots(&#10;        rows=1, cols=2,&#10;        subplot_titles=('Wind Speed [m/s]', 'Wind Direction [°]'),&#10;        horizontal_spacing=0.15&#10;    )&#10;&#10;    # Create frames for slider&#10;    frames = []&#10;&#10;    for frame_idx in range(max_frames):&#10;        frame_traces = []&#10;&#10;        # ====== SUBPLOT 1: Wind Speed ======&#10;        if 'ff' in sl88_data.variables and frame_idx &lt; len(sl88_data.time):&#10;            ff_sl88 = sl88_data['ff'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=ff_sl88,&#10;                    y=heights_sl88,&#10;                    mode='lines+markers',&#10;                    name='SL88',&#10;                    line=dict(color=colors['SL88'], width=2),&#10;                    marker=dict(size=4, symbol='circle'),&#10;                    legendgroup='SL88',&#10;                    showlegend=True,&#10;                    xaxis='x1',&#10;                    yaxis='y1'&#10;                )&#10;            )&#10;&#10;        if 'ff' in slxr142_data.variables and frame_idx &lt; len(slxr142_data.time):&#10;            ff_slxr142 = slxr142_data['ff'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=ff_slxr142,&#10;                    y=heights_slxr142,&#10;                    mode='lines+markers',&#10;                    name='SLXR142',&#10;                    line=dict(color=colors['SLXR142'], width=2),&#10;                    marker=dict(size=4, symbol='square'),&#10;                    legendgroup='SLXR142',&#10;                    showlegend=True,&#10;                    xaxis='x1',&#10;                    yaxis='y1'&#10;                )&#10;            )&#10;&#10;        # ====== SUBPLOT 2: Wind Direction ======&#10;        if 'dd' in sl88_data.variables and frame_idx &lt; len(sl88_data.time):&#10;            dd_sl88 = sl88_data['dd'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=dd_sl88,&#10;                    y=heights_sl88,&#10;                    mode='markers',&#10;                    name='SL88',&#10;                    marker=dict(size=8, symbol='circle', color=colors['SL88']),&#10;                    legendgroup='SL88',&#10;                    showlegend=False,&#10;                    xaxis='x2',&#10;                    yaxis='y2'&#10;                )&#10;            )&#10;&#10;        if 'dd' in slxr142_data.variables and frame_idx &lt; len(slxr142_data.time):&#10;            dd_slxr142 = slxr142_data['dd'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=dd_slxr142,&#10;                    y=heights_slxr142,&#10;                    mode='markers',&#10;                    name='SLXR142',&#10;                    marker=dict(size=8, symbol='square', color=colors['SLXR142']),&#10;                    legendgroup='SLXR142',&#10;                    showlegend=False,&#10;                    xaxis='x2',&#10;                    yaxis='y2'&#10;                )&#10;            )&#10;&#10;        # Frame names from datetime coordinate - use SL88 time if available, else SLXR142&#10;        if frame_idx &lt; len(sl88_data.time):&#10;            timestamp_str = pd.to_datetime(sl88_data.time.values[frame_idx]).strftime('%Y-%m-%d %H:%M')&#10;        else:&#10;            timestamp_str = pd.to_datetime(slxr142_data.time.values[frame_idx]).strftime('%Y-%m-%d %H:%M')&#10;&#10;        frames.append(go.Frame(data=frame_traces, name=timestamp_str))&#10;&#10;    # Add initial traces (first frame)&#10;    if frames:&#10;        for i, trace in enumerate(frames[0].data):&#10;            col = 1 if i &lt; 2 else 2&#10;            fig.add_trace(trace, row=1, col=col)&#10;&#10;    # Update layout with synchronized y-axes&#10;    fig.update_xaxes(title_text=&quot;Wind Speed [m/s]&quot;, range=[0, 10], row=1, col=1)&#10;    fig.update_xaxes(title_text=&quot;Wind Direction [°]&quot;, range=[0, 360], row=1, col=2)&#10;    fig.update_yaxes(title_text=&quot;Height [m]&quot;, range=[0, 1000], row=1, col=1)&#10;    fig.update_yaxes(title_text=&quot;Height [m]&quot;, range=[0, 1000], row=1, col=2, matches='y')&#10;&#10;    # Create slider&#10;    sliders = [dict(&#10;        active=0,&#10;        yanchor=&quot;top&quot;,&#10;        y=-0.2,&#10;        xanchor=&quot;left&quot;,&#10;        x=0.0,&#10;        currentvalue=dict(prefix=&quot;Time: &quot;, visible=True, xanchor=&quot;left&quot;),&#10;        pad=dict(b=10, t=50),&#10;        len=0.9,&#10;        steps=[&#10;            dict(&#10;                args=[[frame.name],&#10;                      dict(frame=dict(duration=300, redraw=True), mode=&quot;immediate&quot;, transition=dict(duration=300))],&#10;                label=frame.name,&#10;                method=&quot;animate&quot;&#10;            )&#10;            for frame in frames&#10;        ]&#10;    )]&#10;&#10;    # Play/Pause Buttons&#10;    updatemenus = [dict(&#10;        type=&quot;buttons&quot;,&#10;        direction=&quot;left&quot;,&#10;        x=0.0,&#10;        y=-0.15,&#10;        xanchor=&quot;left&quot;,&#10;        yanchor=&quot;top&quot;,&#10;        pad=dict(r=10, t=70),&#10;        showactive=False,&#10;        buttons=[&#10;            dict(label=&quot;▶ Play&quot;, method=&quot;animate&quot;, args=[None,&#10;                                                         dict(frame=dict(duration=500, redraw=True), fromcurrent=True,&#10;                                                              mode=&quot;immediate&quot;, transition=dict(duration=300))]),&#10;            dict(label=&quot;⏸ Pause&quot;, method=&quot;animate&quot;, args=[[None],&#10;                                                          dict(frame=dict(duration=0, redraw=False), mode=&quot;immediate&quot;,&#10;                                                               transition=dict(duration=0))])&#10;        ]&#10;    )]&#10;&#10;    fig.update_layout(&#10;        title=dict(&#10;            text='LIDAR Wind Measurements Comparison: SL88 vs SLXR142&lt;br&gt;&lt;sub&gt;October 15, 2017 12:00 - October 16, 2017 12:00&lt;/sub&gt;',&#10;            x=0.5,&#10;            xanchor='center'&#10;        ),&#10;        height=650,&#10;        width=1400,&#10;        hovermode='closest',&#10;        updatemenus=updatemenus,&#10;        sliders=sliders,&#10;        legend=dict(x=1.02, y=1, xanchor='left', yanchor='top')&#10;    )&#10;&#10;    fig.frames = frames&#10;&#10;    if save_plot:&#10;        plot_path = os.path.join(confg.dir_PLOTS, 'lidar_comparison_interactive.html')&#10;        os.makedirs(confg.dir_PLOTS, exist_ok=True)&#10;        fig.write_html(plot_path)&#10;        print(f&quot;Interactive plot saved to: {plot_path}&quot;)&#10;&#10;    print(&quot;Interactive plot created!&quot;)&#10;    return fig&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    &quot;&quot;&quot;&#10;    Main function to run LIDAR data analysis&#10;    &quot;&quot;&quot;&#10;    print(&quot;Starting LIDAR data analysis...&quot;)&#10;&#10;    # First check if processed merged files already exist&#10;    # sl88_data, slxr142_data = read_merged_lidar_data()&#10;&#10;    # If merged files don't exist or are empty, process original data&#10;    # if sl88_data is None or slxr142_data is None:&#10;        # print(&quot;Merged files not found or faulty - processing original data...&quot;)&#10;    sl88_data = read_edit_original_lidar_data(confg.lidar_sl88, &quot;SL88&quot;)&#10;    slxr142_data = read_edit_original_lidar_data(confg.lidar_slxr142, &quot;SLXR142&quot;)&#10;&#10;    # else:&#10;    #    print(&quot;Merged files already exist - skipping processing!&quot;)&#10;&#10;    # Create comparison plot&#10;    if sl88_data is not None or slxr142_data is not None:&#10;        plot_lidar_comparison(sl88_data, slxr142_data)&#10;    else:&#10;        print(&quot;No data available for plot!&quot;)&#10;&#10;    print(&quot;LIDAR analysis completed!&quot;)&#10;" />
              <option name="updatedContent" value="import glob&#10;import os&#10;&#10;import numpy as np&#10;import pandas as pd&#10;import plotly.graph_objects as go&#10;import xarray as xr&#10;from plotly.subplots import make_subplots&#10;&#10;import confg&#10;&#10;&#10;def read_edit_original_lidar_data(data_path, instrument_name):&#10;    &quot;&quot;&quot;&#10;    Reads LIDAR data from specified path, merges both files, converts time to datetime coordinates,&#10;    filters to desired time window and 30-min intervals, then saves as merged file&#10;&#10;    Parameters:&#10;    -----------&#10;    data_path : str&#10;        Path to LIDAR data folder&#10;    instrument_name : str&#10;        Instrument name (e.g. 'SL88' or 'SLXR142')&#10;&#10;    Returns:&#10;    --------&#10;    xarray.Dataset&#10;        Processed and saved dataset with datetime coordinates, filtered to time window and 30-min intervals&#10;    &quot;&quot;&quot;&#10;&#10;    # Find all NetCDF files in folder (exclude merged files)&#10;    nc_files = [f for f in glob.glob(os.path.join(data_path, &quot;*.nc&quot;))&#10;                if 'merged' not in os.path.basename(f)]&#10;&#10;    if not nc_files:&#10;        print(f&quot;No NetCDF files found in {data_path}&quot;)&#10;        return None&#10;&#10;    print(f&quot;Found {len(nc_files)} files for {instrument_name}&quot;)&#10;&#10;    try:&#10;        # Load all files and merge them&#10;        # Use decode_times=False to avoid cftime issues&#10;        datasets = []&#10;        height_array = None  # Store height array separately&#10;&#10;        for file in sorted(nc_files):&#10;            ds = xr.open_dataset(file, decode_times=False)&#10;&#10;            # Store height from first file&#10;            if height_array is None and 'height' in ds.variables:&#10;                height_array = ds['height'].values&#10;&#10;            # Add instrument info&#10;            ds.attrs['instrument'] = instrument_name&#10;            datasets.append(ds)&#10;&#10;        # Combine along time index dimension&#10;        combined_ds = xr.concat(datasets, dim='NUMBER_OF_SCANS')&#10;&#10;        if 'time' in combined_ds.variables:&#10;            # Convert time values to datetime&#10;            datetime_values = pd.to_datetime(combined_ds.time, unit='s')&#10;&#10;            # Remove old time variable&#10;            combined_ds = combined_ds.drop_vars(&quot;time&quot;)&#10;&#10;            # Rename dimension and set datetime as coordinate&#10;            combined_ds = combined_ds.rename({'NUMBER_OF_SCANS': 'time'})&#10;            combined_ds = combined_ds.assign_coords({'time': datetime_values})&#10;&#10;        # Add height as 1D coordinate (not as concatenated variable)&#10;        if height_array is not None:&#10;            combined_ds = combined_ds.assign_coords({'height_m': ('NUMBER_OF_GATES', height_array)})&#10;        &#10;        # Fix height coordinate: use first timestamp's height values as constant coordinate&#10;        if 'height' in combined_ds.variables and len(combined_ds['height'].dims) == 2:&#10;            print(f&quot;  Converting 2D height coordinate to 1D using first timestamp...&quot;)&#10;            # Get height values from first timestamp&#10;            first_timestamp_heights = combined_ds['height'].isel(time=0).values&#10;            &#10;            # Remove the old 2D height variable&#10;            combined_ds = combined_ds.drop_vars('height')&#10;            &#10;            # Add as 1D coordinate&#10;            combined_ds = combined_ds.assign_coords({'height': ('NUMBER_OF_GATES', first_timestamp_heights)})&#10;            print(f&quot;  Height coordinate converted from 2D to 1D with {len(first_timestamp_heights)} levels&quot;)&#10;&#10;        print(f&quot;  Full time range: {combined_ds.time.values[0]} to {combined_ds.time.values[-1]}&quot;)&#10;&#10;        # Define time window: 2017-10-15 12:00 to 2017-10-16 12:00&#10;        start_time = '2017-10-15 12:00:00'&#10;        end_time = '2017-10-16 12:00:00'&#10;&#10;        # Time window filtering first&#10;        filtered_ds = combined_ds.sel(time=slice(start_time, end_time))&#10;&#10;        # Create 30-min interval timestamps (aligned to :00 and :30)&#10;        target_times = pd.date_range(start=start_time, end=end_time, freq='30min')&#10;&#10;        print(f&quot;  Selecting nearest timesteps to 30-min intervals...&quot;)&#10;&#10;        # Select nearest timesteps to target times&#10;        try:&#10;            subset_ds = filtered_ds.sel(time=target_times, method='nearest', tolerance='10min')&#10;        except ValueError:&#10;            print()&#10;&#10;        print(f&quot;  After filtering: {len(subset_ds.time)} timesteps (30-min intervals)&quot;)&#10;        print(f&quot;  Final time range: {subset_ds.time.values[0]} to {subset_ds.time.values[-1]}&quot;)&#10;&#10;        # Rename NUMBER_OF_GATES dimension to height for clarity&#10;        if 'NUMBER_OF_GATES' in subset_ds.dims:&#10;            subset_ds = subset_ds.rename({'NUMBER_OF_GATES': 'height'})&#10;            print(f&quot;  Renamed dimension NUMBER_OF_GATES to height&quot;)&#10;        &#10;        # Ensure height coordinate is properly set as 1D after filtering&#10;        if 'height' in subset_ds.coords and len(subset_ds.coords['height'].dims) == 1:&#10;            print(f&quot;  Height coordinate is now 1D with {len(subset_ds.coords['height'])} levels&quot;)&#10;            print(f&quot;  Height range: {subset_ds.coords['height'].min().values:.2f} - {subset_ds.coords['height'].max().values:.2f} m&quot;)&#10;&#10;        # Save path - use appropriate filename based on instrument&#10;        if instrument_name == 'SL88':&#10;            output_path = os.path.join(data_path, 'sl88_merged.nc')&#10;        elif instrument_name == 'SLXR142':&#10;            output_path = os.path.join(data_path, 'slxr142_merged.nc')&#10;        else:&#10;            output_path = os.path.join(data_path, f'{instrument_name.lower()}_merged.nc')&#10;&#10;        # Save as NetCDF&#10;        subset_ds.to_netcdf(output_path)&#10;        print(f&quot;  Saved to: {output_path}&quot;)&#10;&#10;        return subset_ds&#10;&#10;    except Exception as e:&#10;        print(f&quot;Error loading {instrument_name} data: {e}&quot;)&#10;        import traceback&#10;        traceback.print_exc()&#10;        return None&#10;&#10;&#10;def read_merged_lidar_data():&#10;    &quot;&quot;&quot;&#10;    Reads both pre-processed merged LIDAR files if they exist&#10;&#10;    Returns:&#10;    --------&#10;    tuple&#10;        (sl88_data, slxr142_data) - Two xarray Datasets or None if not available&#10;    &quot;&quot;&quot;&#10;    sl88_data, slxr142_data = None, None&#10;&#10;    # Check and load SL88 merged file&#10;    if os.path.exists(confg.lidar_sl88_merged_path):&#10;        print(f&quot;Loading existing SL88 merged file: {confg.lidar_sl88_merged_path}&quot;)&#10;        try:&#10;            sl88_data = xr.open_dataset(confg.lidar_sl88_merged_path)&#10;            print(&#10;                f&quot;  SL88: {len(sl88_data.time)} timesteps from {sl88_data.time.values[0]} to {sl88_data.time.values[-1]}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;Error loading SL88 merged file: {e}&quot;)&#10;            sl88_data = None&#10;    else:&#10;        print(f&quot;SL88 merged file not found: {confg.lidar_sl88_merged_path}&quot;)&#10;&#10;    # Check and load SLXR142 merged file&#10;    if os.path.exists(confg.lidar_slxr142_merged_path):&#10;        print(f&quot;Loading existing SLXR142 merged file: {confg.lidar_slxr142_merged_path}&quot;)&#10;        try:&#10;            slxr142_data = xr.open_dataset(confg.lidar_slxr142_merged_path)&#10;            print(&#10;                f&quot;  SLXR142: {len(slxr142_data.time)} timesteps from {slxr142_data.time.values[0]} to {slxr142_data.time.values[-1]}&quot;)&#10;        except Exception as e:&#10;            print(f&quot;Error loading SLXR142 merged file: {e}&quot;)&#10;            slxr142_data = None&#10;    else:&#10;        print(f&quot;SLXR142 merged file not found: {confg.lidar_slxr142_merged_path}&quot;)&#10;&#10;    return sl88_data, slxr142_data&#10;&#10;&#10;def plot_lidar_comparison(sl88_data, slxr142_data, save_plot=True):&#10;    &quot;&quot;&quot;&#10;    Creates interactive Plotly comparison plots with time slider for wind speed and direction of both LIDAR systems&#10;    Data is expected to be already filtered to desired time range and 30-min intervals&#10;&#10;    Parameters:&#10;    -----------&#10;    sl88_data : xarray.Dataset&#10;        SL88 LIDAR data (with datetime coordinates, already filtered)&#10;    slxr142_data : xarray.Dataset&#10;        SLXR142 LIDAR data (with datetime coordinates, already filtered)&#10;    save_plot : bool&#10;        Whether to save the plot&#10;    &quot;&quot;&quot;&#10;&#10;    if sl88_data is None or slxr142_data is None:&#10;        print(&quot;One or both datasets are empty!&quot;)&#10;        return&#10;&#10;    # Colors for both systems&#10;    colors = {'SL88': '#1f77b4', 'SLXR142': '#ff7f0e'}&#10;    # extract heights&#10;    heights_sl88 = sl88_data['height_m'].values&#10;    heights_slxr142 = slxr142_data['height_m'].values&#10;&#10;    print(f&quot;SL88: {len(sl88_data.time)} timesteps from {sl88_data.time.values[0]} to {sl88_data.time.values[-1]}&quot;)&#10;    print(f&quot;SLXR142: {len(slxr142_data.time)} timesteps from {slxr142_data.time.values[0]} to {slxr142_data.time.values[-1]}&quot;)&#10;&#10;    # Use full time series for slider&#10;    max_frames = max(len(sl88_data.time), len(slxr142_data.time))&#10;&#10;    print(f&quot;Creating interactive plot with {max_frames} timesteps...&quot;)&#10;&#10;    # Create subplots: [Wind Speed | Wind Direction]&#10;    fig = make_subplots(&#10;        rows=1, cols=2,&#10;        subplot_titles=('Wind Speed [m/s]', 'Wind Direction [°]'),&#10;        horizontal_spacing=0.15&#10;    )&#10;&#10;    # Create frames for slider&#10;    frames = []&#10;&#10;    for frame_idx in range(max_frames):&#10;        frame_traces = []&#10;&#10;        # ====== SUBPLOT 1: Wind Speed ======&#10;        if 'ff' in sl88_data.variables and frame_idx &lt; len(sl88_data.time):&#10;            ff_sl88 = sl88_data['ff'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=ff_sl88,&#10;                    y=heights_sl88,&#10;                    mode='lines+markers',&#10;                    name='SL88',&#10;                    line=dict(color=colors['SL88'], width=2),&#10;                    marker=dict(size=4, symbol='circle'),&#10;                    legendgroup='SL88',&#10;                    showlegend=True,&#10;                    xaxis='x1',&#10;                    yaxis='y1'&#10;                )&#10;            )&#10;&#10;        if 'ff' in slxr142_data.variables and frame_idx &lt; len(slxr142_data.time):&#10;            ff_slxr142 = slxr142_data['ff'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=ff_slxr142,&#10;                    y=heights_slxr142,&#10;                    mode='lines+markers',&#10;                    name='SLXR142',&#10;                    line=dict(color=colors['SLXR142'], width=2),&#10;                    marker=dict(size=4, symbol='square'),&#10;                    legendgroup='SLXR142',&#10;                    showlegend=True,&#10;                    xaxis='x1',&#10;                    yaxis='y1'&#10;                )&#10;            )&#10;&#10;        # ====== SUBPLOT 2: Wind Direction ======&#10;        if 'dd' in sl88_data.variables and frame_idx &lt; len(sl88_data.time):&#10;            dd_sl88 = sl88_data['dd'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=dd_sl88,&#10;                    y=heights_sl88,&#10;                    mode='markers',&#10;                    name='SL88',&#10;                    marker=dict(size=8, symbol='circle', color=colors['SL88']),&#10;                    legendgroup='SL88',&#10;                    showlegend=False,&#10;                    xaxis='x2',&#10;                    yaxis='y2'&#10;                )&#10;            )&#10;&#10;        if 'dd' in slxr142_data.variables and frame_idx &lt; len(slxr142_data.time):&#10;            dd_slxr142 = slxr142_data['dd'].isel(time=frame_idx).values&#10;            frame_traces.append(&#10;                go.Scatter(&#10;                    x=dd_slxr142,&#10;                    y=heights_slxr142,&#10;                    mode='markers',&#10;                    name='SLXR142',&#10;                    marker=dict(size=8, symbol='square', color=colors['SLXR142']),&#10;                    legendgroup='SLXR142',&#10;                    showlegend=False,&#10;                    xaxis='x2',&#10;                    yaxis='y2'&#10;                )&#10;            )&#10;&#10;        # Frame names from datetime coordinate - use SL88 time if available, else SLXR142&#10;        if frame_idx &lt; len(sl88_data.time):&#10;            timestamp_str = pd.to_datetime(sl88_data.time.values[frame_idx]).strftime('%Y-%m-%d %H:%M')&#10;        else:&#10;            timestamp_str = pd.to_datetime(slxr142_data.time.values[frame_idx]).strftime('%Y-%m-%d %H:%M')&#10;&#10;        frames.append(go.Frame(data=frame_traces, name=timestamp_str))&#10;&#10;    # Add initial traces (first frame)&#10;    if frames:&#10;        for i, trace in enumerate(frames[0].data):&#10;            col = 1 if i &lt; 2 else 2&#10;            fig.add_trace(trace, row=1, col=col)&#10;&#10;    # Update layout with synchronized y-axes&#10;    fig.update_xaxes(title_text=&quot;Wind Speed [m/s]&quot;, range=[0, 10], row=1, col=1)&#10;    fig.update_xaxes(title_text=&quot;Wind Direction [°]&quot;, range=[0, 360], row=1, col=2)&#10;    fig.update_yaxes(title_text=&quot;Height [m]&quot;, range=[0, 1000], row=1, col=1)&#10;    fig.update_yaxes(title_text=&quot;Height [m]&quot;, range=[0, 1000], row=1, col=2, matches='y')&#10;&#10;    # Create slider&#10;    sliders = [dict(&#10;        active=0,&#10;        yanchor=&quot;top&quot;,&#10;        y=-0.2,&#10;        xanchor=&quot;left&quot;,&#10;        x=0.0,&#10;        currentvalue=dict(prefix=&quot;Time: &quot;, visible=True, xanchor=&quot;left&quot;),&#10;        pad=dict(b=10, t=50),&#10;        len=0.9,&#10;        steps=[&#10;            dict(&#10;                args=[[frame.name],&#10;                      dict(frame=dict(duration=300, redraw=True), mode=&quot;immediate&quot;, transition=dict(duration=300))],&#10;                label=frame.name,&#10;                method=&quot;animate&quot;&#10;            )&#10;            for frame in frames&#10;        ]&#10;    )]&#10;&#10;    # Play/Pause Buttons&#10;    updatemenus = [dict(&#10;        type=&quot;buttons&quot;,&#10;        direction=&quot;left&quot;,&#10;        x=0.0,&#10;        y=-0.15,&#10;        xanchor=&quot;left&quot;,&#10;        yanchor=&quot;top&quot;,&#10;        pad=dict(r=10, t=70),&#10;        showactive=False,&#10;        buttons=[&#10;            dict(label=&quot;▶ Play&quot;, method=&quot;animate&quot;, args=[None,&#10;                                                         dict(frame=dict(duration=500, redraw=True), fromcurrent=True,&#10;                                                              mode=&quot;immediate&quot;, transition=dict(duration=300))]),&#10;            dict(label=&quot;⏸ Pause&quot;, method=&quot;animate&quot;, args=[[None],&#10;                                                          dict(frame=dict(duration=0, redraw=False), mode=&quot;immediate&quot;,&#10;                                                               transition=dict(duration=0))])&#10;        ]&#10;    )]&#10;&#10;    fig.update_layout(&#10;        title=dict(&#10;            text='LIDAR Wind Measurements Comparison: SL88 vs SLXR142&lt;br&gt;&lt;sub&gt;October 15, 2017 12:00 - October 16, 2017 12:00&lt;/sub&gt;',&#10;            x=0.5,&#10;            xanchor='center'&#10;        ),&#10;        height=650,&#10;        width=1400,&#10;        hovermode='closest',&#10;        updatemenus=updatemenus,&#10;        sliders=sliders,&#10;        legend=dict(x=1.02, y=1, xanchor='left', yanchor='top')&#10;    )&#10;&#10;    fig.frames = frames&#10;&#10;    if save_plot:&#10;        plot_path = os.path.join(confg.dir_PLOTS, 'lidar_comparison_interactive.html')&#10;        os.makedirs(confg.dir_PLOTS, exist_ok=True)&#10;        fig.write_html(plot_path)&#10;        print(f&quot;Interactive plot saved to: {plot_path}&quot;)&#10;&#10;    print(&quot;Interactive plot created!&quot;)&#10;    return fig&#10;&#10;&#10;if __name__ == &quot;__main__&quot;:&#10;    &quot;&quot;&quot;&#10;    Main function to run LIDAR data analysis&#10;    &quot;&quot;&quot;&#10;    print(&quot;Starting LIDAR data analysis...&quot;)&#10;&#10;    # First check if processed merged files already exist&#10;    # sl88_data, slxr142_data = read_merged_lidar_data()&#10;&#10;    # If merged files don't exist or are empty, process original data&#10;    # if sl88_data is None or slxr142_data is None:&#10;        # print(&quot;Merged files not found or faulty - processing original data...&quot;)&#10;    sl88_data = read_edit_original_lidar_data(confg.lidar_sl88, &quot;SL88&quot;)&#10;    slxr142_data = read_edit_original_lidar_data(confg.lidar_slxr142, &quot;SLXR142&quot;)&#10;&#10;    # else:&#10;    #    print(&quot;Merged files already exist - skipping processing!&quot;)&#10;&#10;    # Create comparison plot&#10;    if sl88_data is not None or slxr142_data is not None:&#10;        plot_lidar_comparison(sl88_data, slxr142_data)&#10;    else:&#10;        print(&quot;No data available for plot!&quot;)&#10;&#10;    print(&quot;LIDAR analysis completed!&quot;)&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/tests/test_read_in_hatpro_radiosonde.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/tests/test_read_in_hatpro_radiosonde.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;written entirely by Claude AI&#10;Test suite for read_in_hatpro_radiosonde.py&#10;&#10;Tests all active functions with mock data to ensure proper functionality.&#10;&quot;&quot;&quot;&#10;&#10;import numpy as np&#10;import pandas as pd&#10;import pytest&#10;import xarray as xr&#10;from unittest.mock import patch&#10;import tempfile&#10;import os&#10;&#10;# Import the functions to test&#10;import sys&#10;sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))&#10;from read_in_hatpro_radiosonde import (&#10;    calc_vars_hatpro_w_pressure,&#10;    calc_vars_radiosonde,&#10;    edit_vars,&#10;    read_radiosonde_csv,&#10;    convert_to_dataset,&#10;    plot_height_levels,&#10;    read_radiosonde_dataset,&#10;    read_hatpro,&#10;    interpolate_hatpro_arome&#10;)&#10;&#10;&#10;class TestCalcVarsHatproWPressure:&#10;    &quot;&quot;&quot;Test the calc_vars_hatpro_w_pressure function&quot;&quot;&quot;&#10;&#10;    def test_calc_vars_basic(self):&#10;        &quot;&quot;&quot;Test basic calculation of potential temperature and density&quot;&quot;&quot;&#10;        # Create mock dataset&#10;        time = pd.date_range('2017-10-15 12:00', periods=5, freq='30min')&#10;        height = np.array([100, 200, 300, 400, 500])&#10;&#10;        ds = xr.Dataset({&#10;            'temp': (['time', 'height'], np.random.uniform(10, 20, (5, 5))),&#10;            'p': (['time', 'height'], np.random.uniform(900, 1000, (5, 5))),&#10;            'absolute_humidity': (['time', 'height'], np.random.uniform(5, 15, (5, 5)))&#10;        }, coords={'time': time, 'height': height})&#10;&#10;        result = calc_vars_hatpro_w_pressure(ds)&#10;&#10;        # Check that new variables were created&#10;        assert 'th' in result.variables&#10;        assert 'rho' in result.variables&#10;        assert 'q' in result.variables&#10;&#10;        # Check that values are reasonable&#10;        assert np.all(result['th'].values &gt; 200)  # Potential temp should be &gt; 280K&#10;        assert np.all(result['rho'].values &gt; 0.8)  # Density should be positive and reasonable&#10;        assert np.all(result['rho'].values &lt; 1.5)&#10;        assert np.all(result['q'].values &gt; 0)  # Specific humidity should be positive&#10;        assert np.all(result['q'].values &lt; 0.03)  # and less than 3%&#10;&#10;&#10;class TestCalcVarsRadiosonde:&#10;    &quot;&quot;&quot;Test the calc_vars_radiosonde function&quot;&quot;&quot;&#10;&#10;    def test_calc_vars_radiosonde_basic(self):&#10;        &quot;&quot;&quot;Test radiosonde variable calculations&quot;&quot;&quot;&#10;        # Create mock dataframe&#10;        df = pd.DataFrame({&#10;            'temp': np.array([15, 10, 5, 0, -5]),&#10;            'p': np.array([1000, 900, 800, 700, 600]),&#10;            'Td': np.array([10, 5, 0, -5, -10]),&#10;            'z': np.array([100, 500, 1000, 1500, 2000])&#10;        })&#10;&#10;        result = calc_vars_radiosonde(df)&#10;&#10;        # Check that new variables were created&#10;        assert 'th' in result.columns&#10;        assert 'rho' in result.columns&#10;        assert 'q' in result.columns&#10;&#10;        # Check that values are reasonable&#10;        assert np.all(result['th'] &gt; 150)  # Potential temp&#10;        assert np.all(result['rho'] &gt; 0.3)  # Density&#10;        assert np.all(result['rho'] &lt; 1.5)&#10;        assert np.all(result['q'] &gt; 0)  # Specific humidity&#10;&#10;&#10;class TestEditVars:&#10;    &quot;&quot;&quot;Test the edit_vars function&quot;&quot;&quot;&#10;&#10;    def test_edit_vars_conversion(self):&#10;        &quot;&quot;&quot;Test variable editing and unit conversions&quot;&quot;&quot;&#10;        df = pd.DataFrame({&#10;            'time': [0, 1, 2],&#10;            'temperature': [288.15, 283.15, 278.15],  # Kelvin&#10;            'dewpoint': [285.15, 280.15, 275.15],  # Kelvin&#10;            'pressure': [100000, 90000, 80000],  # Pa&#10;            'geopotential height': [100, 500, 1000],  # m&#10;            'wind direction': [180, 200, 220],&#10;            'windspeed': [5, 10, 15],&#10;            'latitude offset': [0, 0, 0],&#10;            'longitude offset': [0, 0, 0]&#10;        })&#10;&#10;        result = edit_vars(df)&#10;&#10;        # Check conversions&#10;        assert 'temp' in result.columns&#10;        assert 'Td' in result.columns&#10;        assert 'p' in result.columns&#10;        assert 'z' in result.columns&#10;&#10;        # Check values&#10;        assert np.allclose(result['temp'].values, [15, 10, 5])  # Celsius&#10;        assert np.allclose(result['Td'].values, [12, 7, 2])  # Celsius&#10;        assert np.allclose(result['p'].values, [1000, 900, 800])  # hPa&#10;&#10;        # Check that original columns were dropped&#10;        assert 'temperature' not in result.columns&#10;        assert 'dewpoint' not in result.columns&#10;        assert 'pressure' not in result.columns&#10;        assert 'time' not in result.columns&#10;&#10;&#10;class TestReadRadiosondeCsv:&#10;    &quot;&quot;&quot;Test the read_radiosonde_csv function&quot;&quot;&quot;&#10;&#10;    def test_read_radiosonde_csv(self):&#10;        &quot;&quot;&quot;Test reading radiosonde CSV file&quot;&quot;&quot;&#10;        # Create temporary CSV file&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:&#10;            f.write(&quot;# Comment line 1\n&quot;)&#10;            f.write(&quot;# Comment line 2\n&quot;)&#10;            f.write(&quot;# Comment line 3\n&quot;)&#10;            f.write(&quot;# Comment line 4\n&quot;)&#10;            f.write(&quot;# Comment line 5\n&quot;)&#10;            f.write(&quot;time,temperature,dewpoint,pressure,geopotential height,wind direction,windspeed,latitude offset,longitude offset\n&quot;)&#10;            f.write(&quot;0,288.15,285.15,100000,100,180,5,0,0\n&quot;)&#10;            f.write(&quot;1,283.15,280.15,90000,500,200,10,0,0\n&quot;)&#10;            f.write(&quot;2,278.15,275.15,80000,1000,220,15,0,0\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            result = read_radiosonde_csv(temp_file)&#10;&#10;            # Check that data was read correctly&#10;            assert len(result) == 3&#10;            assert 'temp' in result.columns&#10;            assert 'p' in result.columns&#10;            assert 'z' in result.columns&#10;&#10;            # Check conversions&#10;            assert np.allclose(result['temp'].values, [15, 10, 5])&#10;            assert np.allclose(result['p'].values, [1000, 900, 800])&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;&#10;class TestConvertToDataset:&#10;    &quot;&quot;&quot;Test the convert_to_dataset function&quot;&quot;&quot;&#10;&#10;    def test_convert_to_dataset_basic(self):&#10;        &quot;&quot;&quot;Test conversion from DataFrame to xarray Dataset&quot;&quot;&quot;&#10;        # Create mock dataframe with index&#10;        df = pd.DataFrame({&#10;            'temp': [np.nan, 15, 10, 5],  # First value is NaN (will be dropped)&#10;            'p': [np.nan, 1000, 900, 800],&#10;            'th': [np.nan, 290, 292, 294],&#10;            'q': [np.nan, 0.01, 0.008, 0.006],&#10;            'rho': [np.nan, 1.2, 1.1, 1.0]&#10;        })&#10;        df.index = [0, 100, 500, 1000]  # Height index&#10;&#10;        result = convert_to_dataset(df)&#10;&#10;        # Check that it's a dataset&#10;        assert isinstance(result, xr.Dataset)&#10;&#10;        # Check that first NaN value was dropped&#10;        assert len(result['height']) == 3&#10;&#10;        # Check that all variables are present&#10;        assert 'temp' in result.variables&#10;        assert 'p' in result.variables&#10;        assert 'th' in result.variables&#10;&#10;        # Check attributes&#10;        assert 'units' in result['th'].attrs&#10;        assert 'units' in result['q'].attrs&#10;        assert 'units' in result['rho'].attrs&#10;&#10;&#10;class TestPlotHeightLevels:&#10;    &quot;&quot;&quot;Test the plot_height_levels function&quot;&quot;&quot;&#10;&#10;    @patch('matplotlib.pyplot.show')&#10;    @patch('matplotlib.pyplot.savefig')&#10;    def test_plot_height_levels(self, mock_savefig, mock_show):&#10;        &quot;&quot;&quot;Test plotting of height levels&quot;&quot;&quot;&#10;        # Create mock height arrays&#10;        arome_heights = np.linspace(600, 3000, 50)&#10;        icon_heights = np.linspace(600, 3000, 45)&#10;        um_heights = np.linspace(600, 3000, 40)&#10;        wrf_heights = np.linspace(600, 3000, 48)&#10;        radio_heights = np.array([600, 800, 1000, 1500, 2000, 2500, 3000])&#10;        hatpro_heights = np.array([600, 700, 900, 1200, 1600, 2100, 2700])&#10;&#10;        # Should not raise any errors&#10;        plot_height_levels(&#10;            arome_heights, icon_heights, um_heights,&#10;            wrf_heights, radio_heights, hatpro_heights&#10;        )&#10;&#10;        # Check that plot functions were called&#10;        mock_show.assert_called_once()&#10;&#10;&#10;class TestReadRadiosondeDataset:&#10;    &quot;&quot;&quot;Test the read_radiosonde_dataset function&quot;&quot;&quot;&#10;&#10;    def test_read_radiosonde_dataset_direct(self):&#10;        &quot;&quot;&quot;Test reading radiosonde dataset with direct height&quot;&quot;&quot;&#10;        # Create temporary dataset&#10;        height_idx = np.arange(10)&#10;        z_values = np.array([600, 700, 850, 1000, 1200, 1500, 1800, 2100, 2500, 3000])&#10;&#10;        ds = xr.Dataset({&#10;            'temp': (['height'], np.random.uniform(5, 15, 10)),&#10;            'p': (['height'], np.linspace(1000, 700, 10)),&#10;            'z': (['height'], z_values),&#10;            'th': (['height'], np.random.uniform(285, 295, 10)),&#10;            'q': (['height'], np.random.uniform(0.005, 0.015, 10)),&#10;            'rho': (['height'], np.linspace(1.2, 0.9, 10))&#10;        }, coords={'height': height_idx})&#10;&#10;        with tempfile.NamedTemporaryFile(delete=False, suffix='.nc') as f:&#10;            temp_file = f.name&#10;&#10;        try:&#10;            ds.to_netcdf(temp_file)&#10;&#10;            with patch('confg.radiosonde_dataset', temp_file):&#10;                result = read_radiosonde_dataset(height_as_z_coord=&quot;direct&quot;)&#10;&#10;                # Check that height was set to z values&#10;                assert np.allclose(result['height'].values, z_values)&#10;                assert result['height'].attrs['units'] == 'm'&#10;        finally:&#10;            if os.path.exists(temp_file):&#10;                os.unlink(temp_file)&#10;&#10;    def test_read_radiosonde_dataset_above_terrain(self):&#10;        &quot;&quot;&quot;Test reading radiosonde dataset with height above terrain&quot;&quot;&quot;&#10;        # Create temporary dataset&#10;        height_idx = np.arange(10)&#10;        z_values = np.array([600, 700, 850, 1000, 1200, 1500, 1800, 2100, 2500, 3000])&#10;&#10;        ds = xr.Dataset({&#10;            'temp': (['height'], np.random.uniform(5, 15, 10)),&#10;            'p': (['height'], np.linspace(1000, 700, 10)),&#10;            'z': (['height'], z_values),&#10;            'th': (['height'], np.random.uniform(285, 295, 10)),&#10;            'q': (['height'], np.random.uniform(0.005, 0.015, 10)),&#10;            'rho': (['height'], np.linspace(1.2, 0.9, 10))&#10;        }, coords={'height': height_idx})&#10;&#10;        with tempfile.NamedTemporaryFile(delete=False, suffix='.nc') as f:&#10;            temp_file = f.name&#10;&#10;        try:&#10;            ds.to_netcdf(temp_file)&#10;&#10;            with patch('confg.radiosonde_dataset', temp_file):&#10;                result = read_radiosonde_dataset(height_as_z_coord=&quot;above_terrain&quot;)&#10;&#10;                # Check that height starts at 1m&#10;                assert result['height'].values[0] == 1&#10;                # Check that height differences are preserved&#10;                expected_heights = z_values - z_values[0] + 1&#10;                assert np.allclose(result['height'].values, expected_heights)&#10;        finally:&#10;            if os.path.exists(temp_file):&#10;                os.unlink(temp_file)&#10;&#10;&#10;class TestReadHatpro:&#10;    &quot;&quot;&quot;Test the read_hatpro function&quot;&quot;&quot;&#10;&#10;    def test_read_hatpro_temp(self):&#10;        &quot;&quot;&quot;Test reading HATPRO temperature data&quot;&quot;&quot;&#10;        # Create temporary CSV file with HATPRO format&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_temp.csv') as f:&#10;            f.write(&quot;rawdate;&quot; + &quot;;&quot;.join([f&quot;h{i:02d}&quot; for i in range(1, 40)]) + &quot;\n&quot;)&#10;            # Add some data rows&#10;            for hour in range(12, 15):&#10;                time_str = f&quot;2017-10-15 {hour:02d}:00:00&quot;&#10;                values = &quot;;&quot;.join([str(280 + i * 0.5 + np.random.random()) for i in range(39)])&#10;                f.write(f&quot;{time_str};{values}\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            # Mock the hatpro_vertical_levels config&#10;            with patch('confg.hatpro_vertical_levels',&#10;                      {&quot;height&quot;: [str(i * 50) for i in range(1, 40)]}):&#10;                result = read_hatpro(temp_file)&#10;&#10;                # Check structure&#10;                assert isinstance(result, xr.Dataset)&#10;                assert 'th' in result.variables&#10;                assert 'time' in result.coords&#10;                assert 'height_level' in result.coords&#10;&#10;                # Check dimensions&#10;                assert len(result['height_level']) == 39&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;    def test_read_hatpro_humidity(self):&#10;        &quot;&quot;&quot;Test reading HATPRO humidity data&quot;&quot;&quot;&#10;        # Create temporary CSV file with HATPRO format&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_humidity.csv') as f:&#10;            f.write(&quot;rawdate;&quot; + &quot;;&quot;.join([f&quot;h{i:02d}&quot; for i in range(1, 40)]) + &quot;\n&quot;)&#10;            # Add some data rows&#10;            for hour in range(12, 15):&#10;                time_str = f&quot;2017-10-15 {hour:02d}:00:00&quot;&#10;                values = &quot;;&quot;.join([str(8 - i * 0.1 + np.random.random()) for i in range(39)])&#10;                f.write(f&quot;{time_str};{values}\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            # Mock the hatpro_vertical_levels config&#10;            with patch('confg.hatpro_vertical_levels',&#10;                      {&quot;height&quot;: [str(i * 50) for i in range(1, 40)]}):&#10;                result = read_hatpro(temp_file)&#10;&#10;                # Check structure&#10;                assert isinstance(result, xr.Dataset)&#10;                assert 'humidity' in result.variables&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;&#10;class TestInterpolateHatproArome:&#10;    &quot;&quot;&quot;Test the interpolate_hatpro_arome function&quot;&quot;&quot;&#10;&#10;    def test_interpolate_hatpro_arome_basic(self):&#10;        &quot;&quot;&quot;Test interpolation of HATPRO to AROME levels&quot;&quot;&quot;&#10;        # Create mock HATPRO dataset&#10;        time = pd.date_range('2017-10-15 12:00', '2017-10-16 12:00', freq='10min')&#10;        height_hatpro = np.array([50, 100, 200, 350, 550, 800, 1100, 1450, 1850])&#10;&#10;        hatpro = xr.Dataset({&#10;            'temp': (['time', 'height'], np.random.uniform(10, 20, (len(time), len(height_hatpro)))),&#10;            'humidity': (['time', 'height'], np.random.uniform(5, 15, (len(time), len(height_hatpro))))&#10;        }, coords={'time': time, 'height': height_hatpro})&#10;&#10;        # Create mock AROME dataset&#10;        time_arome = pd.date_range('2017-10-15 12:00', '2017-10-16 12:00', freq='30min')&#10;        height_arome = np.array([5.1, 30, 60, 100, 150, 220, 310, 420, 560, 730, 930, 1160, 1430])&#10;&#10;        arome = xr.Dataset({&#10;            'p': (['time', 'height'], np.random.uniform(900, 1000, (len(time_arome), len(height_arome))))&#10;        }, coords={'time': time_arome, 'height': height_arome})&#10;&#10;        # Mock config values&#10;        mock_all_points = {&#10;            'ibk_uni': {'height': 612},&#10;            'ibk_villa': {'height': 579}&#10;        }&#10;        with patch('confg.ALL_POINTS', mock_all_points), \&#10;             patch('confg.hatpro_calced_vars', tempfile.mktemp(suffix='.nc')):&#10;&#10;            # Should not raise errors&#10;            interpolate_hatpro_arome(hatpro, arome)&#10;&#10;&#10;class TestIntegration:&#10;    &quot;&quot;&quot;Integration tests for the full workflow&quot;&quot;&quot;&#10;&#10;    def test_full_radiosonde_workflow(self):&#10;        &quot;&quot;&quot;Test complete radiosonde processing workflow&quot;&quot;&quot;&#10;        # Create mock CSV data&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:&#10;            f.write(&quot;# Comment\n&quot; * 5)&#10;            f.write(&quot;time,temperature,dewpoint,pressure,geopotential height,wind direction,windspeed,latitude offset,longitude offset\n&quot;)&#10;            for i in range(10):&#10;                temp_k = 288.15 - i * 2&#10;                td_k = temp_k - 5&#10;                p_pa = 100000 - i * 5000&#10;                z = 100 + i * 200&#10;                f.write(f&quot;{i},{temp_k},{td_k},{p_pa},{z},180,5,0,0\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            # Read CSV&#10;            df = read_radiosonde_csv(temp_file)&#10;            assert len(df) == 10&#10;&#10;            # Calculate variables&#10;            df_calc = calc_vars_radiosonde(df)&#10;            assert 'th' in df_calc.columns&#10;            assert 'rho' in df_calc.columns&#10;            assert 'q' in df_calc.columns&#10;&#10;            # Convert to dataset&#10;            df_calc.index = df_calc['z'].values&#10;            ds = convert_to_dataset(df_calc)&#10;            assert isinstance(ds, xr.Dataset)&#10;&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;&#10;if __name__ == '__main__':&#10;    pytest.main([__file__, '-v', '--tb=short'])&#10;&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;written entirely by Claude AI&#10;Test suite for read_in_hatpro_radiosonde.py&#10;&#10;Tests all active functions with mock data to ensure proper functionality.&#10;&quot;&quot;&quot;&#10;&#10;import numpy as np&#10;import pandas as pd&#10;import pytest&#10;import xarray as xr&#10;from unittest.mock import patch&#10;import tempfile&#10;import os&#10;&#10;# Import the functions to test&#10;import sys&#10;sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))&#10;from read_in_hatpro_radiosonde import (&#10;    calc_vars_hatpro_w_pressure,&#10;    calc_vars_radiosonde,&#10;    edit_vars,&#10;    read_radiosonde_csv,&#10;    convert_to_dataset,&#10;    plot_height_levels,&#10;    read_radiosonde_dataset,&#10;    read_hatpro,&#10;    interpolate_hatpro_arome&#10;)&#10;&#10;&#10;class TestCalcVarsHatproWPressure:&#10;    &quot;&quot;&quot;Test the calc_vars_hatpro_w_pressure function&quot;&quot;&quot;&#10;&#10;    def test_calc_vars_basic(self):&#10;        &quot;&quot;&quot;Test basic calculation of potential temperature and density&quot;&quot;&quot;&#10;        # Create mock dataset&#10;        time = pd.date_range('2017-10-15 12:00', periods=5, freq='30min')&#10;        height = np.array([100, 200, 300, 400, 500])&#10;&#10;        ds = xr.Dataset({&#10;            'temp': (['time', 'height'], np.random.uniform(10, 20, (5, 5))),&#10;            'p': (['time', 'height'], np.random.uniform(900, 1000, (5, 5))),&#10;            'absolute_humidity': (['time', 'height'], np.random.uniform(5, 15, (5, 5)))&#10;        }, coords={'time': time, 'height': height})&#10;&#10;        result = calc_vars_hatpro_w_pressure(ds)&#10;&#10;        # Check that new variables were created&#10;        assert 'th' in result.variables&#10;        assert 'rho' in result.variables&#10;        assert 'q' in result.variables&#10;&#10;        # Check that values are reasonable&#10;        assert np.all(result['th'].values &gt; 200)  # Potential temp should be &gt; 280K&#10;        assert np.all(result['rho'].values &gt; 0.8)  # Density should be positive and reasonable&#10;        assert np.all(result['rho'].values &lt; 1.5)&#10;        assert np.all(result['q'].values &gt; 0)  # Specific humidity should be positive&#10;        assert np.all(result['q'].values &lt; 0.03)  # and less than 3%&#10;&#10;&#10;class TestCalcVarsRadiosonde:&#10;    &quot;&quot;&quot;Test the calc_vars_radiosonde function&quot;&quot;&quot;&#10;&#10;    def test_calc_vars_radiosonde_basic(self):&#10;        &quot;&quot;&quot;Test radiosonde variable calculations&quot;&quot;&quot;&#10;        # Create mock dataframe&#10;        df = pd.DataFrame({&#10;            'temp': np.array([15, 10, 5, 0, -5]),&#10;            'p': np.array([1000, 900, 800, 700, 600]),&#10;            'Td': np.array([10, 5, 0, -5, -10]),&#10;            'z': np.array([100, 500, 1000, 1500, 2000])&#10;        })&#10;&#10;        result = calc_vars_radiosonde(df)&#10;&#10;        # Check that new variables were created&#10;        assert 'th' in result.columns&#10;        assert 'rho' in result.columns&#10;        assert 'q' in result.columns&#10;&#10;        # Check that values are reasonable&#10;        assert np.all(result['th'] &gt; 150)  # Potential temp&#10;        assert np.all(result['rho'] &gt; 0.3)  # Density&#10;        assert np.all(result['rho'] &lt; 1.5)&#10;        assert np.all(result['q'] &gt; 0)  # Specific humidity&#10;&#10;&#10;class TestEditVars:&#10;    &quot;&quot;&quot;Test the edit_vars function&quot;&quot;&quot;&#10;&#10;    def test_edit_vars_conversion(self):&#10;        &quot;&quot;&quot;Test variable editing and unit conversions&quot;&quot;&quot;&#10;        df = pd.DataFrame({&#10;            'time': [0, 1, 2],&#10;            'temperature': [288.15, 283.15, 278.15],  # Kelvin&#10;            'dewpoint': [285.15, 280.15, 275.15],  # Kelvin&#10;            'pressure': [100000, 90000, 80000],  # Pa&#10;            'geopotential height': [100, 500, 1000],  # m&#10;            'wind direction': [180, 200, 220],&#10;            'windspeed': [5, 10, 15],&#10;            'latitude offset': [0, 0, 0],&#10;            'longitude offset': [0, 0, 0]&#10;        })&#10;&#10;        result = edit_vars(df)&#10;&#10;        # Check conversions&#10;        assert 'temp' in result.columns&#10;        assert 'Td' in result.columns&#10;        assert 'p' in result.columns&#10;        assert 'z' in result.columns&#10;&#10;        # Check values&#10;        assert np.allclose(result['temp'].values, [15, 10, 5])  # Celsius&#10;        assert np.allclose(result['Td'].values, [12, 7, 2])  # Celsius&#10;        assert np.allclose(result['p'].values, [1000, 900, 800])  # hPa&#10;&#10;        # Check that original columns were dropped&#10;        assert 'temperature' not in result.columns&#10;        assert 'dewpoint' not in result.columns&#10;        assert 'pressure' not in result.columns&#10;        assert 'time' not in result.columns&#10;&#10;&#10;class TestReadRadiosondeCsv:&#10;    &quot;&quot;&quot;Test the read_radiosonde_csv function&quot;&quot;&quot;&#10;&#10;    def test_read_radiosonde_csv(self):&#10;        &quot;&quot;&quot;Test reading radiosonde CSV file&quot;&quot;&quot;&#10;        # Create temporary CSV file&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:&#10;            f.write(&quot;# Comment line 1\n&quot;)&#10;            f.write(&quot;# Comment line 2\n&quot;)&#10;            f.write(&quot;# Comment line 3\n&quot;)&#10;            f.write(&quot;# Comment line 4\n&quot;)&#10;            f.write(&quot;# Comment line 5\n&quot;)&#10;            f.write(&quot;time,temperature,dewpoint,pressure,geopotential height,wind direction,windspeed,latitude offset,longitude offset\n&quot;)&#10;            f.write(&quot;0,288.15,285.15,100000,100,180,5,0,0\n&quot;)&#10;            f.write(&quot;1,283.15,280.15,90000,500,200,10,0,0\n&quot;)&#10;            f.write(&quot;2,278.15,275.15,80000,1000,220,15,0,0\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            result = read_radiosonde_csv(temp_file)&#10;&#10;            # Check that data was read correctly&#10;            assert len(result) == 3&#10;            assert 'temp' in result.columns&#10;            assert 'p' in result.columns&#10;            assert 'z' in result.columns&#10;&#10;            # Check conversions&#10;            assert np.allclose(result['temp'].values, [15, 10, 5])&#10;            assert np.allclose(result['p'].values, [1000, 900, 800])&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;&#10;class TestConvertToDataset:&#10;    &quot;&quot;&quot;Test the convert_to_dataset function&quot;&quot;&quot;&#10;&#10;    def test_convert_to_dataset_basic(self):&#10;        &quot;&quot;&quot;Test conversion from DataFrame to xarray Dataset&quot;&quot;&quot;&#10;        # Create mock dataframe with index&#10;        df = pd.DataFrame({&#10;            'temp': [np.nan, 15, 10, 5],  # First value is NaN (will be dropped)&#10;            'p': [np.nan, 1000, 900, 800],&#10;            'th': [np.nan, 290, 292, 294],&#10;            'q': [np.nan, 0.01, 0.008, 0.006],&#10;            'rho': [np.nan, 1.2, 1.1, 1.0]&#10;        })&#10;        df.index = [0, 100, 500, 1000]  # Height index&#10;&#10;        result = convert_to_dataset(df)&#10;&#10;        # Check that it's a dataset&#10;        assert isinstance(result, xr.Dataset)&#10;&#10;        # Check that first NaN value was dropped&#10;        assert len(result['height']) == 3&#10;&#10;        # Check that all variables are present&#10;        assert 'temp' in result.variables&#10;        assert 'p' in result.variables&#10;        assert 'th' in result.variables&#10;&#10;        # Check attributes&#10;        assert 'units' in result['th'].attrs&#10;        assert 'units' in result['q'].attrs&#10;        assert 'units' in result['rho'].attrs&#10;&#10;&#10;class TestPlotHeightLevels:&#10;    &quot;&quot;&quot;Test the plot_height_levels function&quot;&quot;&quot;&#10;&#10;    @patch('matplotlib.pyplot.show')&#10;    @patch('matplotlib.pyplot.savefig')&#10;    def test_plot_height_levels(self, mock_savefig, mock_show):&#10;        &quot;&quot;&quot;Test plotting of height levels&quot;&quot;&quot;&#10;        # Create mock height arrays&#10;        arome_heights = np.linspace(600, 3000, 50)&#10;        icon_heights = np.linspace(600, 3000, 45)&#10;        um_heights = np.linspace(600, 3000, 40)&#10;        wrf_heights = np.linspace(600, 3000, 48)&#10;        radio_heights = np.array([600, 800, 1000, 1500, 2000, 2500, 3000])&#10;        hatpro_heights = np.array([600, 700, 900, 1200, 1600, 2100, 2700])&#10;&#10;        # Should not raise any errors&#10;        plot_height_levels(&#10;            arome_heights, icon_heights, um_heights,&#10;            wrf_heights, radio_heights, hatpro_heights&#10;        )&#10;&#10;        # Check that plot functions were called&#10;        mock_show.assert_called_once()&#10;&#10;&#10;class TestReadRadiosondeDataset:&#10;    &quot;&quot;&quot;Test the read_radiosonde_dataset function&quot;&quot;&quot;&#10;&#10;    def test_read_radiosonde_dataset_direct(self):&#10;        &quot;&quot;&quot;Test reading radiosonde dataset with direct height&quot;&quot;&quot;&#10;        # Create temporary dataset&#10;        height_idx = np.arange(10)&#10;        z_values = np.array([600, 700, 850, 1000, 1200, 1500, 1800, 2100, 2500, 3000])&#10;&#10;        ds = xr.Dataset({&#10;            'temp': (['height'], np.random.uniform(5, 15, 10)),&#10;            'p': (['height'], np.linspace(1000, 700, 10)),&#10;            'z': (['height'], z_values),&#10;            'th': (['height'], np.random.uniform(285, 295, 10)),&#10;            'q': (['height'], np.random.uniform(0.005, 0.015, 10)),&#10;            'rho': (['height'], np.linspace(1.2, 0.9, 10))&#10;        }, coords={'height': height_idx})&#10;&#10;        with tempfile.NamedTemporaryFile(delete=False, suffix='.nc') as f:&#10;            temp_file = f.name&#10;&#10;        try:&#10;            ds.to_netcdf(temp_file)&#10;&#10;            with patch('confg.radiosonde_dataset', temp_file):&#10;                result = read_radiosonde_dataset(height_as_z_coord=&quot;direct&quot;)&#10;&#10;                # Check that height was set to z values&#10;                assert np.allclose(result['height'].values, z_values)&#10;                assert result['height'].attrs['units'] == 'm'&#10;        finally:&#10;            if os.path.exists(temp_file):&#10;                os.unlink(temp_file)&#10;&#10;    def test_read_radiosonde_dataset_above_terrain(self):&#10;        &quot;&quot;&quot;Test reading radiosonde dataset with height above terrain&quot;&quot;&quot;&#10;        # Create temporary dataset&#10;        height_idx = np.arange(10)&#10;        z_values = np.array([600, 700, 850, 1000, 1200, 1500, 1800, 2100, 2500, 3000])&#10;&#10;        ds = xr.Dataset({&#10;            'temp': (['height'], np.random.uniform(5, 15, 10)),&#10;            'p': (['height'], np.linspace(1000, 700, 10)),&#10;            'z': (['height'], z_values),&#10;            'th': (['height'], np.random.uniform(285, 295, 10)),&#10;            'q': (['height'], np.random.uniform(0.005, 0.015, 10)),&#10;            'rho': (['height'], np.linspace(1.2, 0.9, 10))&#10;        }, coords={'height': height_idx})&#10;&#10;        with tempfile.NamedTemporaryFile(delete=False, suffix='.nc') as f:&#10;            temp_file = f.name&#10;&#10;        try:&#10;            ds.to_netcdf(temp_file)&#10;&#10;            with patch('confg.radiosonde_dataset', temp_file):&#10;                result = read_radiosonde_dataset(height_as_z_coord=&quot;above_terrain&quot;)&#10;&#10;                # Check that height starts at 1m&#10;                assert result['height'].values[0] == 1&#10;                # Check that height differences are preserved&#10;                expected_heights = z_values - z_values[0] + 1&#10;                assert np.allclose(result['height'].values, expected_heights)&#10;        finally:&#10;            if os.path.exists(temp_file):&#10;                os.unlink(temp_file)&#10;&#10;&#10;class TestReadHatpro:&#10;    &quot;&quot;&quot;Test the read_hatpro function&quot;&quot;&quot;&#10;&#10;    def test_read_hatpro_temp(self):&#10;        &quot;&quot;&quot;Test reading HATPRO temperature data&quot;&quot;&quot;&#10;        # Create temporary CSV file with HATPRO format&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_temp.csv') as f:&#10;            f.write(&quot;rawdate;&quot; + &quot;;&quot;.join([f&quot;h{i:02d}&quot; for i in range(1, 40)]) + &quot;\n&quot;)&#10;            # Add some data rows&#10;            for hour in range(12, 15):&#10;                time_str = f&quot;2017-10-15 {hour:02d}:00:00&quot;&#10;                values = &quot;;&quot;.join([str(280 + i * 0.5 + np.random.random()) for i in range(39)])&#10;                f.write(f&quot;{time_str};{values}\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            # Mock the hatpro_vertical_levels config&#10;            with patch('confg.hatpro_vertical_levels',&#10;                      {&quot;height&quot;: [str(i * 50) for i in range(1, 40)]}):&#10;                result = read_hatpro(temp_file)&#10;&#10;                # Check structure&#10;                assert isinstance(result, xr.Dataset)&#10;                assert 'th' in result.variables&#10;                assert 'time' in result.coords&#10;                assert 'height_level' in result.coords&#10;&#10;                # Check dimensions&#10;                assert len(result['height_level']) == 39&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;    def test_read_hatpro_humidity(self):&#10;        &quot;&quot;&quot;Test reading HATPRO humidity data&quot;&quot;&quot;&#10;        # Create temporary CSV file with HATPRO format&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='_humidity.csv') as f:&#10;            f.write(&quot;rawdate;&quot; + &quot;;&quot;.join([f&quot;h{i:02d}&quot; for i in range(1, 40)]) + &quot;\n&quot;)&#10;            # Add some data rows&#10;            for hour in range(12, 15):&#10;                time_str = f&quot;2017-10-15 {hour:02d}:00:00&quot;&#10;                values = &quot;;&quot;.join([str(8 - i * 0.1 + np.random.random()) for i in range(39)])&#10;                f.write(f&quot;{time_str};{values}\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            # Mock the hatpro_vertical_levels config&#10;            with patch('confg.hatpro_vertical_levels',&#10;                      {&quot;height&quot;: [str(i * 50) for i in range(1, 40)]}):&#10;                result = read_hatpro(temp_file)&#10;&#10;                # Check structure&#10;                assert isinstance(result, xr.Dataset)&#10;                assert 'humidity' in result.variables&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;&#10;class TestInterpolateHatproArome:&#10;    &quot;&quot;&quot;Test the interpolate_hatpro_arome function&quot;&quot;&quot;&#10;&#10;    def test_interpolate_hatpro_arome_basic(self):&#10;        &quot;&quot;&quot;Test interpolation of HATPRO to AROME levels&quot;&quot;&quot;&#10;        # Create mock HATPRO dataset&#10;        time = pd.date_range('2017-10-15 12:00', '2017-10-16 12:00', freq='10min')&#10;        height_hatpro = np.array([50, 100, 200, 350, 550, 800, 1100, 1450, 1850])&#10;&#10;        hatpro = xr.Dataset({&#10;            'temp': (['time', 'height'], np.random.uniform(10, 20, (len(time), len(height_hatpro)))),&#10;            'humidity': (['time', 'height'], np.random.uniform(5, 15, (len(time), len(height_hatpro))))&#10;        }, coords={'time': time, 'height': height_hatpro})&#10;&#10;        # Create mock AROME dataset&#10;        time_arome = pd.date_range('2017-10-15 12:00', '2017-10-16 12:00', freq='30min')&#10;        height_arome = np.array([5.1, 30, 60, 100, 150, 220, 310, 420, 560, 730, 930, 1160, 1430])&#10;&#10;        arome = xr.Dataset({&#10;            'p': (['time', 'height'], np.random.uniform(900, 1000, (len(time_arome), len(height_arome))))&#10;        }, coords={'time': time_arome, 'height': height_arome})&#10;&#10;        # Mock config values&#10;        mock_all_points = {&#10;            'ibk_uni': {'height': 612},&#10;            'ibk_villa': {'height': 579}&#10;        }&#10;        with patch('confg.ALL_POINTS', mock_all_points), \&#10;             patch('confg.hatpro_calced_vars', tempfile.mktemp(suffix='.nc')):&#10;&#10;            # Should not raise errors&#10;            interpolate_hatpro_arome(hatpro, arome)&#10;&#10;&#10;class TestIntegration:&#10;    &quot;&quot;&quot;Integration tests for the full workflow&quot;&quot;&quot;&#10;&#10;    def test_full_radiosonde_workflow(self):&#10;        &quot;&quot;&quot;Test complete radiosonde processing workflow&quot;&quot;&quot;&#10;        # Create mock CSV data&#10;        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.csv') as f:&#10;            f.write(&quot;# Comment\n&quot; * 5)&#10;            f.write(&quot;time,temperature,dewpoint,pressure,geopotential height,wind direction,windspeed,latitude offset,longitude offset\n&quot;)&#10;            for i in range(10):&#10;                temp_k = 288.15 - i * 2&#10;                td_k = temp_k - 5&#10;                p_pa = 100000 - i * 5000&#10;                z = 100 + i * 200&#10;                f.write(f&quot;{i},{temp_k},{td_k},{p_pa},{z},180,5,0,0\n&quot;)&#10;            temp_file = f.name&#10;&#10;        try:&#10;            # Read CSV&#10;            df = read_radiosonde_csv(temp_file)&#10;            assert len(df) == 10&#10;&#10;            # Calculate variables&#10;            df_calc = calc_vars_radiosonde(df)&#10;            assert 'th' in df_calc.columns&#10;            assert 'rho' in df_calc.columns&#10;            assert 'q' in df_calc.columns&#10;&#10;            # Convert to dataset&#10;            df_calc.index = df_calc['z'].values&#10;            ds = convert_to_dataset(df_calc)&#10;            assert isinstance(ds, xr.Dataset)&#10;&#10;        finally:&#10;            os.unlink(temp_file)&#10;&#10;&#10;if __name__ == '__main__':&#10;    pytest.main([__file__, '-v', '--tb=short'])&#10;&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>